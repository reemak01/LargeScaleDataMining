{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "697c9ceb",
   "metadata": {},
   "source": [
    "## Module-3 Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad3c01",
   "metadata": {},
   "source": [
    "This is M3 of 3 modules for the twitter dataset. In this module, we cover the prediction and inference part of Q29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed27c6b9",
   "metadata": {},
   "source": [
    "### QUESTION 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c789ec7",
   "metadata": {},
   "source": [
    "**Describe task**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b84d3",
   "metadata": {},
   "source": [
    "Given a set of tweets text data, we try to find out the entities present in dataset using tweet text using a text rank phrase extraction algorithm along with fuzzy matching. The control parameters are - 'number of top phrases per tweet'; 'minimum frequency of phrase' for it to be considered as an entity; 'minimum number of other close phrases present'. After identifying the entities, we further extract closest keywords to the entity to understand the reference in which it is being talked about. We find the set of tweets talking about this entity and further rank them using a page rank algorithm to generate a tweet summary consisting of 4 top tweets. <br>\n",
    "\n",
    "We predict the closest key phrases, summary and sentiment for entities in each day/ every 10 min on game day (1st Feb) in the dataset. For the game day of 1st Feb, we predict key phrases in each 10 min interval. <br>\n",
    "\n",
    "To run the script, you will need the following:\n",
    "1. './twitter_files_v3/entities.pkl' - Dictionary of entities generated in module 2, also provided in the zip file\n",
    "2. './twitter_files_v3/prediction_data.pkl' - Prediction data generated in module 2, also provided in the zip file\n",
    "3. './glove/glove.6B.100d.txt' - Glove embeddings\n",
    "\n",
    "For each task - **you need to provide 4 inputs** - \n",
    "1. **entity** (from the list of entities),\n",
    "2. **pred_type** (game_day (predicts in last 10 min), reg_day (predicts for entire day))\n",
    "3. **task_type** (from \"sentiment\", \"summary\", \"keywords\")\n",
    "3. **date** (format %Y-%m-%d for reg_day; %Y-%m-%d %H:%M:%S for game day)\n",
    "\n",
    "The 3 task types are: \n",
    "1. 'sentiment': returns the sentiment of the set of tweets for a given entity for the given day or in last 10 min if prediction type is game_day.\n",
    "2. 'summary': returns list of 4 tweets which summarize the tweets for a given entity for the given day or in last 10 min if prediction type is game_day.\n",
    "3. 'keywords': returns list of 10 key phrases that appear in context of a given entity for the given day or in last 10 min if prediction type is game_day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c48d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import orjson as json\n",
    "import time\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "import regex as re\n",
    "import spacy\n",
    "import pytextrank\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import pickle\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "import textblob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "num_cores = 4 #number of cores on your machine\n",
    "num_partitions = 16 #number of partitions to split dataframe\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "import pytz\n",
    "pst_tz = pytz.timezone('America/Los_Angeles')\n",
    "utc_tz = pytz.utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c61350ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load extracted files from M2\n",
    "\n",
    "## list of entities extracted\n",
    "entities = open('./twitter_files_v3/entities_v2.pkl', 'rb')\n",
    "entities_dict = pickle.load(entities)\n",
    "entities = list(entities_dict.values())\n",
    "\n",
    "## prediction data\n",
    "df_file = open('./twitter_files_v3/prediction_data_v2.pkl', 'rb')\n",
    "prediction_df = pickle.load(df_file)\n",
    "\n",
    "## datetime conversions\n",
    "prediction_df['citation_dt_trans'] = prediction_df['citation_datetime'].apply(lambda x: datetime.fromtimestamp(x, pst_tz))\n",
    "prediction_df['utc_datetime'] = prediction_df['citation_datetime'].apply(lambda x: datetime.fromtimestamp(x, utc_tz))\n",
    "prediction_df['date'] = pd.to_datetime(prediction_df['citation_dt_trans']).dt.date\n",
    "prediction_df['datetime'] = prediction_df['citation_dt_trans'].apply(lambda x: str(x).rsplit('-', 1)[0])\n",
    "prediction_df['datetime'] = pd.to_datetime(prediction_df['datetime'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ffd34ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>phrase</th>\n",
       "      <th>clean_phrase</th>\n",
       "      <th>count</th>\n",
       "      <th>entity</th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>citation_datetime</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>citation_dt_trans</th>\n",
       "      <th>utc_datetime</th>\n",
       "      <th>date</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(549327579782840320, #gohawks http://t.co/u1pc...</td>\n",
       "      <td>0.215096</td>\n",
       "      <td>549327579782840320</td>\n",
       "      <td>#gohawks http://t.co/u1pcxpesr8</td>\n",
       "      <td>gohawks</td>\n",
       "      <td>67966</td>\n",
       "      <td>gohawks</td>\n",
       "      <td>gohawks</td>\n",
       "      <td>I &amp;lt;3 our defense! #GoHawks http://t.co/U1pc...</td>\n",
       "      <td>1421518778</td>\n",
       "      <td>i &amp;lt;3 our defense! #gohawks http://t.co/u1pc...</td>\n",
       "      <td>2015-01-17 10:19:38-08:00</td>\n",
       "      <td>2015-01-17 18:19:38+00:00</td>\n",
       "      <td>2015-01-17</td>\n",
       "      <td>2015-01-17 10:19:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(549327579782840320, our defense)</td>\n",
       "      <td>0.117698</td>\n",
       "      <td>549327579782840320</td>\n",
       "      <td>our defense</td>\n",
       "      <td>our defense</td>\n",
       "      <td>51</td>\n",
       "      <td>Not an entity</td>\n",
       "      <td>gohawks</td>\n",
       "      <td>I &amp;lt;3 our defense! #GoHawks http://t.co/U1pc...</td>\n",
       "      <td>1421518778</td>\n",
       "      <td>i &amp;lt;3 our defense! #gohawks http://t.co/u1pc...</td>\n",
       "      <td>2015-01-17 10:19:38-08:00</td>\n",
       "      <td>2015-01-17 18:19:38+00:00</td>\n",
       "      <td>2015-01-17</td>\n",
       "      <td>2015-01-17 10:19:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(549575600210718721, #dogslife http://t.co/gd3...</td>\n",
       "      <td>0.158353</td>\n",
       "      <td>549575600210718721</td>\n",
       "      <td>#dogslife http://t.co/gd3v6vqps5</td>\n",
       "      <td>dogslife</td>\n",
       "      <td>6</td>\n",
       "      <td>NA</td>\n",
       "      <td>gohawks</td>\n",
       "      <td>twelfth dogs are ready! #gohawks #dogslife htt...</td>\n",
       "      <td>1421259536</td>\n",
       "      <td>twelfth dogs are ready! #gohawks #dogslife htt...</td>\n",
       "      <td>2015-01-14 10:18:56-08:00</td>\n",
       "      <td>2015-01-14 18:18:56+00:00</td>\n",
       "      <td>2015-01-14</td>\n",
       "      <td>2015-01-14 10:18:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(549575600210718721, twelfth)</td>\n",
       "      <td>0.157154</td>\n",
       "      <td>549575600210718721</td>\n",
       "      <td>twelfth</td>\n",
       "      <td>twelfth</td>\n",
       "      <td>25</td>\n",
       "      <td>Not an entity</td>\n",
       "      <td>gohawks</td>\n",
       "      <td>twelfth dogs are ready! #gohawks #dogslife htt...</td>\n",
       "      <td>1421259536</td>\n",
       "      <td>twelfth dogs are ready! #gohawks #dogslife htt...</td>\n",
       "      <td>2015-01-14 10:18:56-08:00</td>\n",
       "      <td>2015-01-14 18:18:56+00:00</td>\n",
       "      <td>2015-01-14</td>\n",
       "      <td>2015-01-14 10:18:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(549647876406534144, gohawks)</td>\n",
       "      <td>0.196769</td>\n",
       "      <td>549647876406534144</td>\n",
       "      <td>gohawks</td>\n",
       "      <td>gohawks</td>\n",
       "      <td>67966</td>\n",
       "      <td>gohawks</td>\n",
       "      <td>gohawks</td>\n",
       "      <td>\"Oh no big deal, just NFC West Champs and the ...</td>\n",
       "      <td>1421468519</td>\n",
       "      <td>\"oh no big deal, just nfc west champs and the ...</td>\n",
       "      <td>2015-01-16 20:21:59-08:00</td>\n",
       "      <td>2015-01-17 04:21:59+00:00</td>\n",
       "      <td>2015-01-16</td>\n",
       "      <td>2015-01-16 20:21:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               index         0  \\\n",
       "0  (549327579782840320, #gohawks http://t.co/u1pc...  0.215096   \n",
       "1                  (549327579782840320, our defense)  0.117698   \n",
       "2  (549575600210718721, #dogslife http://t.co/gd3...  0.158353   \n",
       "3                      (549575600210718721, twelfth)  0.157154   \n",
       "4                      (549647876406534144, gohawks)  0.196769   \n",
       "\n",
       "             tweet_id                            phrase clean_phrase  count  \\\n",
       "0  549327579782840320   #gohawks http://t.co/u1pcxpesr8      gohawks  67966   \n",
       "1  549327579782840320                       our defense  our defense     51   \n",
       "2  549575600210718721  #dogslife http://t.co/gd3v6vqps5     dogslife      6   \n",
       "3  549575600210718721                           twelfth      twelfth     25   \n",
       "4  549647876406534144                           gohawks      gohawks  67966   \n",
       "\n",
       "          entity     file                                               text  \\\n",
       "0        gohawks  gohawks  I &lt;3 our defense! #GoHawks http://t.co/U1pc...   \n",
       "1  Not an entity  gohawks  I &lt;3 our defense! #GoHawks http://t.co/U1pc...   \n",
       "2             NA  gohawks  twelfth dogs are ready! #gohawks #dogslife htt...   \n",
       "3  Not an entity  gohawks  twelfth dogs are ready! #gohawks #dogslife htt...   \n",
       "4        gohawks  gohawks  \"Oh no big deal, just NFC West Champs and the ...   \n",
       "\n",
       "   citation_datetime                                         clean_text  \\\n",
       "0         1421518778  i &lt;3 our defense! #gohawks http://t.co/u1pc...   \n",
       "1         1421518778  i &lt;3 our defense! #gohawks http://t.co/u1pc...   \n",
       "2         1421259536  twelfth dogs are ready! #gohawks #dogslife htt...   \n",
       "3         1421259536  twelfth dogs are ready! #gohawks #dogslife htt...   \n",
       "4         1421468519  \"oh no big deal, just nfc west champs and the ...   \n",
       "\n",
       "          citation_dt_trans              utc_datetime        date  \\\n",
       "0 2015-01-17 10:19:38-08:00 2015-01-17 18:19:38+00:00  2015-01-17   \n",
       "1 2015-01-17 10:19:38-08:00 2015-01-17 18:19:38+00:00  2015-01-17   \n",
       "2 2015-01-14 10:18:56-08:00 2015-01-14 18:18:56+00:00  2015-01-14   \n",
       "3 2015-01-14 10:18:56-08:00 2015-01-14 18:18:56+00:00  2015-01-14   \n",
       "4 2015-01-16 20:21:59-08:00 2015-01-17 04:21:59+00:00  2015-01-16   \n",
       "\n",
       "             datetime  \n",
       "0 2015-01-17 10:19:38  \n",
       "1 2015-01-17 10:19:38  \n",
       "2 2015-01-14 10:18:56  \n",
       "3 2015-01-14 10:18:56  \n",
       "4 2015-01-16 20:21:59  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03be6c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['superbowl',\n",
       " 'superbowlxlix',\n",
       " 'patriots',\n",
       " 'seahawks',\n",
       " 'gohawks',\n",
       " 'patriotswin nfl',\n",
       " 'katyperry',\n",
       " 'tom brady',\n",
       " 'seattle',\n",
       " 'halftime',\n",
       " 'football',\n",
       " 'pats',\n",
       " 'superbowlcommercials',\n",
       " 'gopats',\n",
       " 'superbowlsunday',\n",
       " 'seattleseahawks',\n",
       " 'touchdown',\n",
       " 'commercials',\n",
       " 'new england',\n",
       " 'superbowl2015',\n",
       " 'marshawn lynch',\n",
       " 'katy',\n",
       " 'patsnation',\n",
       " 'new england patriots',\n",
       " 'missyelliott',\n",
       " 'budweiser',\n",
       " 'this game',\n",
       " 'sb49 superbowl',\n",
       " 'patriotsnation',\n",
       " 'russell wilson',\n",
       " 'katyperry superbowl',\n",
       " 'packers',\n",
       " 'wilson',\n",
       " 'people',\n",
       " 'halftime show',\n",
       " 'pete carroll',\n",
       " 'america',\n",
       " 'chris matthews',\n",
       " 'beastmode',\n",
       " 'dangerusswilson',\n",
       " 'allyouneedisecuador',\n",
       " 'nflplayoffs',\n",
       " 'lenny kravitz',\n",
       " 'bill belichick',\n",
       " 'last year',\n",
       " 'next year',\n",
       " 'los',\n",
       " 'tom',\n",
       " 'national anthem',\n",
       " 'belichick']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5fd443",
   "metadata": {},
   "source": [
    "### TASK 1: Get key phrases for a given entity in each day or last 10 min on game day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e40258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For each entity get the top 10 descriptive sentiments around it\n",
    "def get_n_close_phrases (data: pd.DataFrame(), entity: str, date: str, pred_type = 'reg_day', n = 10):\n",
    "    try:\n",
    "    \n",
    "        # tweets corresponding to the entity\n",
    "        if(pred_type == 'reg_day'):\n",
    "            data['date'] = data['date'].astype(str)\n",
    "            tmp = data[(data['entity'] == entity) & (data['date'] == date)]\n",
    "            tmp = tmp.drop(['file'], axis =1)\n",
    "            tmp = tmp.drop_duplicates()\n",
    "            \n",
    "        elif(pred_type == 'game_day'):\n",
    "            d = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "            d_prev = d - timedelta(minutes=10)\n",
    "            tmp = data[(data['entity'] == entity) & (data['datetime'] >= d_prev) & \\\n",
    "                  (data['datetime'] <= d)]\n",
    "            tmp = tmp.drop(['file'], axis =1)\n",
    "            tmp = tmp.drop_duplicates()\n",
    "            \n",
    "        tweet_ids = list(set(tmp['tweet_id']))\n",
    "\n",
    "        ## weighted score for other phrases from the tweets\n",
    "        # get relevant tweet data\n",
    "        tmp = data[data['tweet_id'].isin(tweet_ids)]\n",
    "    \n",
    "        # remove rows corresponding to the entity itself\n",
    "        tmp = tmp[tmp['entity'] != entity]\n",
    "        \n",
    "        phrase_counts = tmp.groupby(['clean_phrase']).size().reset_index(name = 'count')\n",
    "        tmp = tmp.drop(['count', 'file'], axis = 1)\n",
    "        tmp = tmp.drop_duplicates()\n",
    "        tmp = pd.merge(tmp, phrase_counts, how = 'left', on = 'clean_phrase')\n",
    "    \n",
    "        # get weighted scores\n",
    "        tmp['weighted_score'] = tmp.apply(lambda row: row[0] * row['count'], axis = 1)\n",
    "    \n",
    "        other_phrases = tmp.groupby(['clean_phrase'])['weighted_score'].sum()\n",
    "        other_phrases = other_phrases.reset_index()\n",
    "        other_phrases = other_phrases.sort_values('weighted_score', ascending = False)\n",
    "        print(other_phrases[:n])\n",
    "        \n",
    "    except:\n",
    "        print(\"Entity not important in the day/interval!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05993906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     clean_phrase  weighted_score\n",
      "96  superbowlxlix       60.837808\n",
      "72       patriots       40.829373\n",
      "35       halftime       18.224625\n",
      "87       seahawks       12.932766\n",
      "93     super bowl       11.643691\n",
      "95      superbowl        5.264207\n",
      "39           katy        4.418890\n",
      "22       el medio        1.691668\n",
      "57   medio tiempo        1.374621\n",
      "71   para su show        0.947675\n"
     ]
    }
   ],
   "source": [
    "get_n_close_phrases(prediction_df, 'katyperry', '2015-01-18', 'reg_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f21e9549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           clean_phrase  weighted_score\n",
      "558           superbowl    26438.615728\n",
      "572       superbowlxlix    19883.578545\n",
      "493                sb49     4912.672432\n",
      "57              america     2285.457083\n",
      "410     national anthem       91.130337\n",
      "568     superbowlsunday       76.039311\n",
      "354    love john legend       51.594259\n",
      "315  john legends voice       49.223582\n",
      "143              church       32.807374\n",
      "361                 man       30.259418\n"
     ]
    }
   ],
   "source": [
    "get_n_close_phrases(prediction_df, 'john legend', '2015-02-01 15:20:00', 'game_day')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e059d9",
   "metadata": {},
   "source": [
    "### TASK 2: Get summary for a given entity in each day or last 10 min on game day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2275d84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('./glove/glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()\n",
    "\n",
    "# function to remove stopwords\n",
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9451054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For each entity get data for the entity and date\n",
    "def get_subset_data (data: pd.DataFrame(), entity: str, date: str, pred_type = 'reg_day'):\n",
    "    try:\n",
    "    \n",
    "        # tweets corresponding to the entity\n",
    "        if(pred_type == 'reg_day'):\n",
    "            data['date'] = data['date'].astype(str)\n",
    "            tmp = data[(data['entity'] == entity) & (data['date'] == date)]\n",
    "            tmp = tmp.drop(['file'], axis =1)\n",
    "            tmp = tmp.drop_duplicates()\n",
    "            \n",
    "        elif(pred_type == 'game_day'):\n",
    "            d = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "            d_prev = d - timedelta(minutes=10)\n",
    "            tmp = data[(data['entity'] == entity) & (data['datetime'] >= d_prev) & \\\n",
    "                  (data['datetime'] <= d)]\n",
    "            tmp = tmp.drop(['file'], axis =1)\n",
    "            tmp = tmp.drop_duplicates()\n",
    "            \n",
    "        tweet_ids = list(set(tmp['tweet_id']))\n",
    "        \n",
    "        ## weighted score for other phrases from the tweets\n",
    "        # get relevant tweet data\n",
    "        tmp = data[data['tweet_id'].isin(tweet_ids)]\n",
    "        tmp = tmp.drop(['count', 'file'], axis = 1)\n",
    "        tmp = tmp.drop_duplicates()\n",
    "    \n",
    "        # remove rows corresponding to the entity itself\n",
    "        tmp = tmp[tmp['entity'] != entity]\n",
    "        phrase_counts = tmp.groupby(['clean_phrase']).size().reset_index(name = 'count')\n",
    "        tmp = pd.merge(tmp, phrase_counts, how = 'left', on = 'clean_phrase')\n",
    "    \n",
    "        # get weighted scores\n",
    "        tmp['weighted_score'] = tmp.apply(lambda row: row[0] * row['count'], axis = 1)\n",
    "        \n",
    "        return tmp\n",
    "    except:\n",
    "        print(\"Not enough data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0b63f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topn_tweets (data: pd.DataFrame(), entity: str, date: str, pred_type = 'reg_day', n=4):\n",
    "    \n",
    "    sub_data = get_subset_data(data, entity, date, pred_type)\n",
    "    \n",
    "    ## process top 100 candidates at max according to important phrases\n",
    "    filter_df = sub_data.groupby(['tweet_id'])['weighted_score'].sum().reset_index()\n",
    "    if(filter_df.shape[0] > 100):\n",
    "        filter_df = filter_df.sort_values('weighted_score', ascending = False)[:100]\n",
    "        tweet_ids = list(set(filter_df['tweet_id']))\n",
    "        sub_data = sub_data[sub_data['tweet_id'].isin(tweet_ids)]\n",
    "        \n",
    "    sentences = list(set(sub_data['text']))\n",
    "    \n",
    "    # clean sentences\n",
    "    clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "    clean_sentences = [s.lower() for s in clean_sentences]\n",
    "    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "    \n",
    "    sentence_vectors = []\n",
    "    for i in clean_sentences:\n",
    "        if len(i) != 0:\n",
    "            v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "        else:\n",
    "            v = np.zeros((100,))\n",
    "        sentence_vectors.append(v)\n",
    "    sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "     \n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    \n",
    "    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        results.append(ranked_sentences[i][1])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e31439ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's over. Let's see if Tom Brady can play better than Peyton Manning against the #Seahawks in the #SuperBowl #NFLPlayoffs #INDvsNE\",\n",
       " \"Andrew Luck has taken the torch from Peyton Manning as the next great #Colts QB that can't get past the #Patriots in the playoffs.\",\n",
       " 'Peyton Manning had his chance last year. Tom Brady gets his chance against the Seahawks this year. #Brady #Patriots',\n",
       " \"Real #Patriots fans should be happy Seattle won, now Tom Brady can do what Peyton Manning couldn't do last year.\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_topn_tweets(prediction_df, 'peyton manning', '2015-01-18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c85c4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John legend 😍 #imean #yes #Superbowl #',\n",
       " 'John Legend was very good! :) #SuperBowl',\n",
       " 'Love John Legend #SuperBowl',\n",
       " 'John Legend getting down #SuperBowl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_topn_tweets(prediction_df, 'john legend', '2015-02-01 15:20:00', 'game_day')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c4149c",
   "metadata": {},
   "source": [
    "### TASK 3: Get sentiment for a given entity in each day or last 10 min on game day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bf83d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment (data: pd.DataFrame(), entity: str, date: str, pred_type = 'reg_day', n=4):\n",
    "    \n",
    "    sub_data = get_subset_data(data, entity, date, pred_type)\n",
    "    sentences = list(set(sub_data['text']))\n",
    "    \n",
    "    # clean sentences\n",
    "    clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "    clean_sentences = [s.lower() for s in clean_sentences]\n",
    "    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "    \n",
    "    polarities_ls = []\n",
    "    for i in clean_sentences:\n",
    "        polarities_ls.append(textblob.TextBlob(i).sentiment.polarity)\n",
    "    sentiment_score =  sum(polarities_ls)/len(polarities_ls)\n",
    "    sentiment = 'Neutral'\n",
    "    if(sentiment_score > 0.05):\n",
    "        sentiment = 'Positive'\n",
    "    if(sentiment_score < -0.05):\n",
    "        sentiment = 'Negative'\n",
    "    \n",
    "    print(\"Overall sentiment is: \", sentiment,\", with score:\", sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22251f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall sentiment is:  Positive , with score: 0.13933725005153577\n"
     ]
    }
   ],
   "source": [
    "get_sentiment(prediction_df, 'peyton manning', '2015-01-18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "396d6911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall sentiment is:  Positive , with score: 0.192128009052351\n"
     ]
    }
   ],
   "source": [
    "get_sentiment(prediction_df, 'john legend', '2015-02-01 15:20:00', 'game_day')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09278df0",
   "metadata": {},
   "source": [
    "### PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3238d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(datetime_string, pred_type):\n",
    "    try:\n",
    "        if(pred_type == 'reg_day'):\n",
    "            return datetime.strptime(datetime_string,\"%Y-%m-%d\")\n",
    "        elif(pred_type == 'game_day'):\n",
    "            return datetime.strptime(datetime_string,\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    except ValueError:\n",
    "        return False\n",
    "        \n",
    "def perform_task (entity, date, task, pred_type):\n",
    "    if(task not in ['sentiment', 'summary', 'keywords'] ):\n",
    "        print(\"Task can only be - sentiment, summary or keywords!\")\n",
    "    elif(pred_type not in ['reg_day', 'game_day']):\n",
    "        print(\"Prediction type can only be - reg_day or game_day!\")\n",
    "    elif(entity not in entities):\n",
    "        print(\"Entity not in data!\")\n",
    "        print(\"Try entities - katyperry, tom brady, rpeyton manning.. (check entities file for more)!\")\n",
    "    elif(validate(date, pred_type) == False):\n",
    "        print(\"Date format not valid!\")\n",
    "        print(\"Try date in format - %Y-%m-%d for reg_day and %Y-%m-%d %H:%M:%S for game_day!\")\n",
    "    else:\n",
    "        if(task == 'keywords'):\n",
    "            get_n_close_phrases(prediction_df, entity, date, pred_type)\n",
    "        elif(task == 'sentiment'):\n",
    "            get_sentiment(prediction_df, entity, date, pred_type)\n",
    "        elif(task == 'summary'):  \n",
    "            results = get_topn_tweets(prediction_df, entity, date, pred_type)\n",
    "            print(results)\n",
    "        else:\n",
    "            print(\"Unknown error occured! Please check input!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "accde4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task to be performed [sentiment, summary, keywords]: sentiment\n",
      "Entity: tom brady\n",
      "Prediction type [reg_day, game_day]: game_day\n",
      "Date: 2015-02-01 15:20:00\n",
      "Overall sentiment is:  Positive , with score: 0.09779048814873557\n"
     ]
    }
   ],
   "source": [
    "task = input('Task to be performed [sentiment, summary, keywords]: ')\n",
    "entity = input('Entity: ')\n",
    "pred_type = input('Prediction type [reg_day, game_day]: ')\n",
    "date = input('Date: ')\n",
    "perform_task(str(entity), str(date), task, pred_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ac3c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
